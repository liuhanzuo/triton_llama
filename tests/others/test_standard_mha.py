# 代码可直接运行，用于测试标准 "MHA 层" 的结果

import torch
import torch.nn as nn
import torch.nn.functional as F


class MultiHeadAttention(nn.Module):
    def __init__(self, embed_dim, num_heads):
        super(MultiHeadAttention, self).__init__()
        assert embed_dim % num_heads == 0, "embed_dim must be divisible by num_heads"

        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads

        # 定义线性变换
        self.query = nn.Linear(embed_dim, embed_dim)
        self.key = nn.Linear(embed_dim, embed_dim)
        self.value = nn.Linear(embed_dim, embed_dim)

        self.out = nn.Linear(embed_dim, embed_dim)

    def forward(self, x, mask=None):
        batch_size, seq_length, embed_dim = x.size()

        # 线性变换并分成多头
        Q = (
            self.query(x)
            .view(batch_size, seq_length, self.num_heads, self.head_dim)
            .transpose(1, 2)
        )  # (batch, heads, seq, head_dim)
        K = (
            self.key(x)
            .view(batch_size, seq_length, self.num_heads, self.head_dim)
            .transpose(1, 2)
        )
        V = (
            self.value(x)
            .view(batch_size, seq_length, self.num_heads, self.head_dim)
            .transpose(1, 2)
        )

        # 计算原始注意力分数, # (batch, heads, seq, seq)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim**0.5)

        # 对 scores 应用 masked
        if mask is not None:
            masked_scores = scores.masked_fill(mask == 0, float("-inf"))

        # 归一化，将注意力权重分数转为概率分布 dim 维度值相加等于，对于2D张量即每行元素值相加等于 1
        attn_scores = F.softmax(masked_scores, dim=-1)  # (batch, heads, seq, seq)
        # 加权求和 (batch, heads, seq, head_dim)
        context = torch.matmul(attn_scores, V)

        context = (
            context.transpose(1, 2).contiguous().view(batch_size, seq_length, embed_dim)
        )
        out = self.out(context)  # 最后的线性变换(batch, seq_length, embed_dim)

        print(
            f"mask 矩阵:\n {mask.squeeze()} \n"
        )  # 使用 torch.squeeze() 函数来移除张量中所有大小为 1 的维度
        print(f"原始的注意力分数矩阵:\n {scores.squeeze()} \n")
        print(f"应用 mask 后的注意力分数矩阵:\n {masked_scores.squeeze()} \n")
        print(
            f"使用 softmax 归一化后的掩码注意力分数矩阵:\n {attn_scores.squeeze()} \n"
        )
        return out


def generate_causal_mask(seq_length):
    """生成一个因果遮罩, 上三角为0, 下三角为1"""
    mask = (
        torch.tril(torch.ones((seq_length, seq_length))).unsqueeze(0).unsqueeze(0)
    )  # (1, 1, seq, seq)
    return mask  # 1表示可见，0表示遮蔽


# 单元测试代码
def test_multihead_attention(
    vocab_size=1000, batch_size=1, seq_length=4, embed_dim=6, num_heads=2
):
    embedding_layer = nn.Embedding(
        vocab_size, embed_dim
    )  # 将 input_ids 转为 embedding 向量
    mha_layer = MultiHeadAttention(embed_dim, num_heads)  # 构建 MHA 模块

    torch.manual_seed(0)
    input_ids = torch.randint(vocab_size, [batch_size, seq_length])  # 构建输入数据
    mask = generate_causal_mask(seq_length)  # 创建注意力 mask, 默认下三角矩阵(张量)

    h = embedding_layer(input_ids)
    output = mha_layer(h, mask)  # MHA 前向传播
    assert output.shape == (batch_size, seq_length, embed_dim), "输出形状不正确"

    # 检查因果遮罩是否有效, 通过设置输入为单位矩阵，观察输出是否遵循因果遮罩
    x_identity = (
        torch.eye(seq_length, embed_dim).unsqueeze(0).repeat(batch_size, 1, 1)
    )  # (batch, seq, embed)
    output_identity = mha_layer(x_identity, mask)

    # 由于输入是单位矩阵，输出应该保持某种结构，可以进行简单的检查
    assert not torch.isnan(output_identity).any(), "输出包含NaN值"

    print("多头注意力输出示例：")
    print(output)


if __name__ == "__main__":
    test_multihead_attention()
