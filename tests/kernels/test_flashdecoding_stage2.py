import torch, math
import triton
import triton.language as tl
from torch.cuda.amp import custom_fwd
from typing import List, Optional, Union
import torch.nn.functional as F


@triton.jit
def _flash_decoding_stage2_kernel(
    Mid_O,  # [batch, head, seq_block_num, head_dim]
    Mid_O_LogExpSum,  # [batch, head, seq_block_num]
    Ouput,  # attention 输出首地址
    mido_batch_stride,
    mido_heads_stride,
    mido_partitions_stride,
    mido_dim_stride,
    mido_les_batch_stride,
    mido_les_heads_stride,
    mido_les_partitions_stride,
    o_bs_stride,
    o_heads_stride,
    o_dim_stride,
    actual_seq_len,  # TODO 支持 PagedAttention 和连续批处理
    BLOCK_DMODEL: tl.constexpr,
    BLOCK_SEQ: tl.constexpr,  # type: ignore
):
    """Reduction (online softmax)"""
    batch_idx = tl.program_id(0)
    head_idx = tl.program_id(1)

    # 初始化偏移
    offs_d = tl.arange(0, BLOCK_DMODEL)

    offs_part_v = (
        batch_idx * mido_batch_stride
        + head_idx * mido_heads_stride
        + offs_d * mido_dim_stride
    )

    offs_part_max = batch_idx * mido_les_batch_stride + head_idx * mido_les_heads_stride

    part_v_ptrs = Mid_O + offs_part_v
    part_max_ptrs = Mid_O_LogExpSum + offs_part_max

    # Reduce kv 分块相关变量值. num_partitions 是 kv 分块数量
    d_i = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)
    m_i = -float("inf")
    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)
    num_partitions = (actual_seq_len + BLOCK_SEQ - 1) // BLOCK_SEQ

    for _ in range(0, num_partitions, 1):
        part_v = tl.load(part_v_ptrs)
        part_max = tl.load(part_max_ptrs)

        # -- 更新局部最大值 和 exp 分子项 p-- #
        m_ij = tl.maximum(part_max, m_i)
        p = tl.exp(part_v - m_ij)

        # -- 计算 alpha = exp(m{j-1} - m{j}) 值 -- #
        alpha = tl.exp(m_i - m_ij)

        # -- 更新归一化项和 attention 输出累加器 -- #
        d_i = d_i * alpha + p

        acc *= alpha
        acc += p * part_v

        # 更新 max 值和指针偏移
        m_i = m_ij
        part_v_ptrs += mido_partitions_stride
        part_max_ptrs += mido_les_partitions_stride

    # -- 更新 attention 输出累加器 -- #
    offs_out = (
        batch_idx * o_bs_stride + head_idx * o_heads_stride + offs_d * o_dim_stride
    )
    tl.store(Ouput + offs_out, acc / d_i)


@torch.no_grad()
def flash_decode_stage2(
    mid_o,
    mid_o_logexpsum,  # 存储每个批次、每个头、每个分区的中间分数输出及 log(sum(exp(scores)))
    atten_output,  # attention 输出首地址
    actual_seq_len,  # kv cache 在 seq_len 维度的最大长度
    PARTITION_SIZE,
):
    HEAD_DIM = mid_o.shape[-1]

    batchs, num_heads = mid_o.shape[0], mid_o.shape[1]
    grid = (batchs, num_heads)

    _flash_decoding_stage2_kernel[grid](
        mid_o,  # [batch, head, seq_block_num, head_dim]
        mid_o_logexpsum,  # [batch, head, seq_block_num]
        atten_output,  # attention 输出首地址
        *mid_o.stride(),
        *mid_o_logexpsum.stride(),
        *atten_output.stride(),
        actual_seq_len,  # TODO 支持 PagedAttention 和连续批处理
        BLOCK_DMODEL=HEAD_DIM,
        BLOCK_SEQ=PARTITION_SIZE,  # type: ignore
        num_warps=4,
        num_stages=2,
    )


import torch


# 定义 PyTorch 对照实现
def pytorch_flash_decode_stage2(mid_o, mid_o_logexpsum, actual_seq_len, partition_size):
    batchs, num_heads, seq_block_num, head_dim = mid_o.shape
    atten_output_pt = torch.zeros(
        batchs, num_heads, head_dim, device="cuda", dtype=torch.float32
    )

    for batch in range(batchs):
        for head in range(num_heads):
            d_i = torch.zeros(head_dim, device="cuda", dtype=torch.float32)
            m_i = torch.full(
                (head_dim,), -float("inf"), device="cuda", dtype=torch.float32
            )  # 初始化为 [head_dim]
            acc = torch.zeros(head_dim, device="cuda", dtype=torch.float32)
            for partition in range(seq_block_num):
                part_v = mid_o[batch, head, partition]  # [head_dim]
                part_max = mid_o_logexpsum[batch, head, partition].item()  # scalar

                # Broadcast part_max to [head_dim] for comparison
                part_max_tensor = torch.full(
                    (head_dim,), part_max, device="cuda", dtype=torch.float32
                )
                m_ij = torch.maximum(part_max_tensor, m_i)  # [head_dim]
                p = torch.exp(part_v - m_ij)  # [head_dim]

                alpha = torch.exp(m_i - m_ij)  # [head_dim]

                d_i = d_i * alpha + p  # [head_dim]
                acc = acc * alpha + p * part_v  # [head_dim]

                m_i = m_ij  # [head_dim]

            # Avoid division by zero by setting zero where d_i is zero
            mask = d_i > 0
            atten_output_pt[batch, head][mask] = acc[mask] / d_i[mask]
            atten_output_pt[batch, head][~mask] = 0.0  # Handle division by zero

    return atten_output_pt


# 设置随机种子以确保可重复性
torch.manual_seed(42)

# 假设头维度为 64，批次为 2，头数为 4，分区数量为 4，实际序列长度为 128，分区大小为 32
batchs, num_heads, seq_block_num, head_dim = (
    2,
    4,
    4,
    64,
)  # head_dim 必须等于 BLOCK_DMODEL_CONST
actual_seq_len = 128
partition_size = 32

# 随机初始化 Mid_O 和 Mid_O_LogExpSum
mid_o = torch.randn(
    batchs, num_heads, seq_block_num, head_dim, device="cuda", dtype=torch.float32
)
mid_o_logexpsum = torch.randn(
    batchs, num_heads, seq_block_num, device="cuda", dtype=torch.float32
)

# 初始化 atten_output
atten_output = torch.zeros(
    batchs, num_heads, head_dim, device="cuda", dtype=torch.float32
)

# 调用修复后的 Triton 函数
flash_decode_stage2(
    mid_o,
    mid_o_logexpsum,
    atten_output,
    actual_seq_len=actual_seq_len,
    PARTITION_SIZE=partition_size,
)

# 调用 PyTorch 实现
pt_atten_output = pytorch_flash_decode_stage2(
    mid_o, mid_o_logexpsum, actual_seq_len, partition_size
)

# 比较 Triton 和 PyTorch 的输出
diff_atten_output = torch.abs(atten_output - pt_atten_output).max()
print(f"Difference in Atten_Output: {diff_atten_output.item()}")

# 断言差异在合理范围内
assert diff_atten_output < 1e-3, "Atten_Output 的差异超出容忍范围"
print("Triton 内核与 PyTorch 实现的数值对比通过。")
